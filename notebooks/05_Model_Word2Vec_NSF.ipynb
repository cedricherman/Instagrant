{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ceddie/anaconda3/envs/insight/lib/python3.6/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "# custom module\n",
    "from utils import utilsvectorizer as uv\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "from pyLDAvis.sklearn import prepare\n",
    "from pyLDAvis.gensim import prepare as prepare_gensim\n",
    "\n",
    "# nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import regexp_tokenize\n",
    "\n",
    "# spacy for language processing\n",
    "import spacy\n",
    "\n",
    "# gensim for alternative models\n",
    "from gensim.models import LdaModel, LdaMulticore, KeyedVectors, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.utils import tokenize, simple_preprocess, lemmatize\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we have the latest version of custom module loaded\n",
    "importlib.reload(uv)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# automatically display LDA vis post prepare call\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "% matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize stopwords from sklearn\n",
    "In my custom tfidf vectorizer, lemmatization is done before removing stopwords. Therefore, 'has' becomes 'ha' post lemmatization and 'has' is a stopword!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sklearn english stopwords\n",
    "sw_sklearn = stop_words.ENGLISH_STOP_WORDS\n",
    "# lemmatization to convert plurals words to singular word\n",
    "Lem = WordNetLemmatizer()\n",
    "# # lemmatize those and update our list\n",
    "# sw_sklearn = set([Lem.lemmatize(s) for s in sw_sklearn ])\n",
    "# # add other common\n",
    "# # sw = sw_sklearn.union(names_corp)\n",
    "# sw = sw_sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client connection to the MongoDb instance running on the local machine\n",
    "client = MongoClient('localhost:27017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database of interest\n",
    "db = client.awards_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor_tmp is a generator\n",
    "cursor_tmp = db.awards_tmp.find({}, {'Amount' : 1,\n",
    "                                     'amount_corrected' : 1,\n",
    "                                     'Agency' : 1,\n",
    "                                     'Branch' : 1,\n",
    "                                     'Abstract' : 1,\n",
    "                                     'company name' : 1,\n",
    "                                     'DUNS' : 1,\n",
    "                                     'title' : 1,\n",
    "                                     'Awards Year' : 1,\n",
    "                                     'Phase':1,\n",
    "                                     'Contract': 1,\n",
    "                                     '_id': False} )\n",
    "# need a new cursor (generator) after you used it\n",
    "df = pd.DataFrame(list(cursor_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Awards year to datetime object\n",
    "df['Awards Year'] = pd.to_datetime(df['Awards Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 167412 entries, 0 to 167411\n",
      "Data columns (total 11 columns):\n",
      "Abstract            167412 non-null object\n",
      "Agency              167405 non-null object\n",
      "Amount              167412 non-null float64\n",
      "Awards Year         167412 non-null datetime64[ns]\n",
      "Branch              82443 non-null object\n",
      "Contract            167412 non-null object\n",
      "DUNS                167412 non-null object\n",
      "Phase               167412 non-null object\n",
      "amount_corrected    167412 non-null float64\n",
      "company name        167412 non-null object\n",
      "title               167412 non-null object\n",
      "dtypes: datetime64[ns](1), float64(2), object(8)\n",
      "memory usage: 14.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark 'N/A' abstract by nan\n",
    "df.loc[df.Abstract == 'N/A', 'Abstract'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate abstract and title\n",
    "df['text'] = df.Abstract.add(df.title, fill_value = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add agency abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_name = {'usda' : 'Department of Agriculture',\n",
    "                'doc' : 'Department of Commerce',\n",
    "                'dod' : 'Department of Defense',\n",
    "                'ed' : 'Department of Education',\n",
    "                'doe' : 'Department of Energy',\n",
    "                'hhs' : 'Department of Health and Human Services',\n",
    "                'dhs' : 'Department of Homeland Security',\n",
    "                'dot' : 'Department of Transportation',\n",
    "                'epa' : 'Environmental Protection Agency',\n",
    "                'nasa' : 'National Aeronautics and Space Administration',\n",
    "                'nsf' : 'National Science Foundation'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_map = {v: k for k, v in agency_name.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['agency_abbr'] = df.Agency.map(agency_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency = df.agency_abbr.value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,\n",
       " Index(['dod', 'hhs', 'nasa', 'nsf', 'doe', 'usda', 'epa', 'doc', 'ed', 'dot',\n",
       "        'dhs'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(agency), agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agency = ['dot', 'dhs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose one Agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling nsf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4406, 13)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag = 'nsf'\n",
    "print('Modeling {}'.format(ag))\n",
    "# discard 0 amount, N/A abstract. Agency shall not be missing and only consider Phase I\n",
    "crit = (df.agency_abbr == ag) & (df.Phase == 'Phase I') & (df.amount_corrected > 0) & (df['Awards Year'] >= '2004')\n",
    "df_analysis = df.loc[crit,:].copy()\n",
    "df_analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target training data\n",
    "y_docs = df_analysis.amount_corrected.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years for stratified train/test\n",
    "strat_years = df_analysis.iloc[:, df_analysis.columns.get_loc('Awards Year') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features training data\n",
    "docs = df_analysis.text.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add lower case to spacy pipeline (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# could use larger model (en_core_web_lg)\n",
    "# SpaCy has 3 pipelines by default, ['parser', 'tagger', 'ner']\n",
    "\n",
    "# Only keep tagger as it also lower case tokens\n",
    "# nlp = spacy.load('en', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lower_case(doc):\n",
    "#     return [d.lower_ for d in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     nlp.remove_pipe('lower_case')\n",
    "# except:\n",
    "#     nlp.add_pipe(lower_case, name='lower_case', last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize text\n",
    "1. Get alphanumeric terms ('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "2. lower case\n",
    "3. remove stopwords\n",
    "4. lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# print(len(STOP_WORDS), len(sw_sklearn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# len([w.lemma_ for w in nlp(docs[0]) if (not w.is_punct) & (not w.is_space) & (not w.is_stop) & (w.pos_ != 'PRON') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Tokenize and compile list of token list\n",
    "# # nlp does lower case each token\n",
    "# docs_list = []\n",
    "\n",
    "# for d in docs:\n",
    "#     # remove punctuation, space, stopwords, pronouns\n",
    "#     docs_list.append([w.lemma_ for w in nlp(d) \n",
    "#                      if (not w.is_punct) & (not w.is_space) & (not w.is_stop) & (w.pos_ != 'PRON') ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_list = []\n",
    "\n",
    "# boolean mask to keep track of out of model documents\n",
    "mask_notempty_doc = np.full_like(docs , True, dtype=bool)\n",
    "\n",
    "for ind, d in enumerate(docs):\n",
    "    # alpahnumeric tokens\n",
    "    tokens = regexp_tokenize(d, '(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "    \n",
    "    # lower case\n",
    "    tokens_low = [t.lower() for t in tokens]\n",
    "    \n",
    "    # remove stopwords and lemmatize\n",
    "    tokens_lem = [ Lem.lemmatize(w) for w in tokens_low if w not in sw_sklearn ]\n",
    "    \n",
    "    # mark this doc as empty\n",
    "    if not tokens_lem:\n",
    "        mask_notempty_doc[ind] = False\n",
    "    else:\n",
    "        docs_list.append(tokens_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([ 1 for d in docs_list if not d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust target and strat_years to remove empty documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4406, (4406,), (4406,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_docs = y_docs[mask_notempty_doc]\n",
    "strat_years = strat_years[mask_notempty_doc]\n",
    "len(docs_list), y_docs.shape, strat_years.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split documents in train/test, labels are optional here. It turns out we know them.\n",
    "X_train, X_test, y_train, y_test = train_test_split( docs_list,\n",
    "                                    y_docs,\n",
    "                                    test_size = 0.3,\n",
    "                                    random_state = 7,\n",
    "                                    stratify = strat_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3084 1322\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive tf-idf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect words and associated word counts\n",
    "dictionary = Dictionary(X_train)\n",
    "# dictionary.filter_extremes(no_below=min_df, no_above=max_df, keep_n=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAG-OF-WORD\n",
    "# list of tuples list: (word_id, count)\n",
    "# word_id is from the dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 20644\n",
      "Number of documents: 3084\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tfidf on our bag-of-word\n",
    "model_tfidf = TfidfModel(corpus=corpus, id2word=id2word, dictionary=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word2Vec model (skip grams, 300d) and create word2vec tf-idf weighted average class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in word2vec, and define functions for parsing tokenized episode summaries.\n",
    "# This model used skip-gram on Google News data, made in 2013 (100B words)\n",
    "word2vec_dir = os.path.join(os.pardir, 'models', 'GoogleNews-vectors-negative300.bin')\n",
    "word2vec = KeyedVectors.load_word2vec_format(word2vec_dir, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(dictionary, tfidf_corpus, word2vec):\n",
    "    \"\"\"\n",
    "        compile list of word vectors and its assciated tf-idf value\n",
    "        from a corpus.\n",
    "        Return a tuple of two list, tfidf values and word vectors\n",
    "        INPUT:\n",
    "            dictionary, used to recover word as a string from its index\n",
    "            tfidf_corpus, tf-idf output from one document/corpus\n",
    "            word2vec, pre-trained model\n",
    "    \"\"\"\n",
    "    vw_not_found = []\n",
    "    # list of tf-idf weights for existing word vectors\n",
    "    # some word do not have a vector representation\n",
    "    tfidf_weights = []\n",
    "\n",
    "    # list of word vectors\n",
    "    wvector_list = []\n",
    "\n",
    "    # collect all word vectors\n",
    "    for d, dtf in tfidf_corpus:\n",
    "\n",
    "        # first element of tfidf_corpus is the word id\n",
    "        # retrieve word (string) from dictionary\n",
    "        term = dictionary.get(d)\n",
    "\n",
    "        # check if word exist in model\n",
    "        if term in word2vec.wv.vocab:\n",
    "\n",
    "            # word2vec returns a numpy array\n",
    "            # store array in list\n",
    "            wvector_list.append(word2vec[term])\n",
    "\n",
    "            # keep track of tfidf coefficient for this word\n",
    "            # 2nd element of tfidf_corpus is the term frequency\n",
    "            tfidf_weights.append(dtf)\n",
    "        else:\n",
    "            vw_not_found.append(term)\n",
    "            \n",
    "    return tfidf_weights, wvector_list, vw_not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vectorizer(corpus, dictionary, model_tfidf, word2vec):\n",
    "    \"\"\"\n",
    "        Document vector based on Tf-idf weighted average of word vectors from that document.\n",
    "        \n",
    "        Return N dimension vector, N being model dimension ,and boolean mask \n",
    "        for missing document vectors (i.e. not a single word had a vector representation)\n",
    "    \"\"\"\n",
    "    # list for document vectors\n",
    "    dvector = []\n",
    "    # list of token list not in word2vec\n",
    "    dvw_notfound = []\n",
    "\n",
    "    # boolean mask to keep track of out of model documents\n",
    "    mask_doc = np.full((len(corpus),), True, dtype=bool)\n",
    "\n",
    "    # analyze each corpus\n",
    "    for ind, corp in enumerate(corpus):\n",
    "\n",
    "        # get tf-idf based on bow corpus\n",
    "        tfidf_corpus = model_tfidf[corp]\n",
    "\n",
    "        # get word vectors and associated tfidf\n",
    "        tfidf_weights, wvector_list, wv_notfound = get_word_vectors(dictionary, tfidf_corpus, word2vec)\n",
    "        \n",
    "        # keep track of word vector not in word2vec\n",
    "        dvw_notfound.append(wv_notfound)\n",
    "        \n",
    "        # skip document/corpus when not a single word found in word2vec model\n",
    "#         if len(wvector_list) < 20:\n",
    "        if not wvector_list:\n",
    "            # flip this record for subsequent filtering\n",
    "            mask_doc[ind] = False\n",
    "            continue\n",
    "\n",
    "        # consolidate all word vectors (2d numpy array, 300xnum_doc)\n",
    "        words_array = np.stack(wvector_list, axis = 1)\n",
    "        \n",
    "# simple mean\n",
    "#         doc_vector = words_array.mean(axis = 1)\n",
    "# OR weighted mean\n",
    "\n",
    "        # create 1d numpy array from tfidf weight list\n",
    "        tfidf_array = np.array(tfidf_weights)\n",
    "\n",
    "        # term-frequency correction factor\n",
    "        # old_total_term / new_total_term\n",
    "        tfidf_array = tfidf_array * len(tfidf_corpus) / len(tfidf_weights)\n",
    "\n",
    "        # document vector (tfidf weighted average of word vector)\n",
    "        doc_vector = np.dot(words_array, tfidf_array) / tfidf_array.sum()\n",
    "        \n",
    "        dvector.append(doc_vector)\n",
    "\n",
    "    # consolidate document vectors\n",
    "    doc_array_train = np.stack(dvector)\n",
    "    \n",
    "    return doc_array_train, mask_doc, dvw_notfound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_array_train, mask_doc, dvw_notfound = document_vectorizer(corpus, dictionary, model_tfidf, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3084, 300)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_array_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of words: 3.7735971456373663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2., 3., 5., 7.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average number of tokens without vector representation \n",
    "num_word = np.array([ len(d) for d in dvw_notfound if not not d])\n",
    "print('Mean number of words:', num_word.mean())\n",
    "np.percentile(num_word, [25, 50, 75, 90 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03242542153047989"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pecentage of document without missing word vectors\n",
    "sum([ 1 for d in dvw_notfound if not d]) / len(dvw_notfound) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check words not found by word2vec per document\n",
    "Mostly empty list as a result of tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sbir', 'fpan', 'resuable']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvw_notfound[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(''.join([ag, '_no_w2v.txt']), 'w') as f:\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerows(dvw_notfound)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check document without any words having a vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(X_train)[~mask_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_jobs = -1, n_estimators = 200, max_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform grid search CV on pipeline\n",
    "_ = rf.fit(doc_array_train, y_train[mask_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score =  0.8693507167700076\n"
     ]
    }
   ],
   "source": [
    "# training set score\n",
    "y_train_pred = rf.predict(doc_array_train)\n",
    "r2_train = r2_score(y_train[mask_doc], y_train_pred)\n",
    "print('Train score = ', r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.72"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at each tree depth\n",
    "sum([estimator.tree_.max_depth for estimator in rf.estimators_ ])/ rf.n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test = [dictionary.doc2bow(doc) for doc in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_array_test, mask_doc_test, dvw_notfound_test = \\\n",
    "                document_vectorizer(corpus_test, dictionary, model_tfidf, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1322, 300)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_array_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dvw_notfound_test[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (np.array(X_test)[~mask_doc_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score =  0.03297532246740975\n"
     ]
    }
   ],
   "source": [
    "# test set score\n",
    "y_test_pred = rf.predict(doc_array_test)\n",
    "r2_test = r2_score(y_test[mask_doc_test], y_test_pred)\n",
    "print('Test score = ', r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/nsf_rf_phase1_w2v.pkl']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pickle model\n",
    "joblib.dump(rf, os.path.join(os.pardir,'models', ''.join([ag, '_rf_phase1_w2v.pkl']) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = ExtraTreesRegressor(n_jobs = -1, n_estimators = 200, min_samples_split = 2, max_features = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# perform grid search CV on pipeline\n",
    "_ = et.fit(doc_array_train, y_train[mask_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score =  1.0\n"
     ]
    }
   ],
   "source": [
    "# training set score\n",
    "y_train_pred = et.predict(doc_array_train)\n",
    "r2_train = r2_score(y_train[mask_doc], y_train_pred)\n",
    "print('Train score = ', r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score =  0.24985069786076386\n"
     ]
    }
   ],
   "source": [
    "# test set score\n",
    "y_test_pred = et.predict(doc_array_test)\n",
    "r2_test = r2_score(y_test[mask_doc_test], y_test_pred)\n",
    "print('Test score = ', r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/nsf_et_phase1_w2v.pkl']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf, os.path.join(os.pardir,'models', ''.join([ag, '_et_phase1_w2v.pkl']) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tv_dtm: output of vectorizer\n",
    "tv_matrix = tv_dtm.toarray()\n",
    "\n",
    "similarity_matrix = cosine_similarity(tv_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process first document\n",
    "first_tfidf = model_tfidf[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_weights = np.array([ weight for _, weight in first_tfidf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of existing word vector\n",
    "tfidf_mask = np.full((len(corpus[0]),), True, dtype=bool)\n",
    "\n",
    "words_array = []\n",
    "for ind, d in enumerate(corpus[0]):\n",
    "    \n",
    "    # first element of d is the word id\n",
    "    # retrieve word (string) from word id\n",
    "    term = dictionary.get(d[0])\n",
    "    \n",
    "    # check if word exist in model\n",
    "    if term in word2vec.wv.vocab:\n",
    "        # word2vec returns a numpy array\n",
    "        # store array in list\n",
    "         words_array.append(word2vec[term])\n",
    "    else:\n",
    "        tfidf_mask[ind] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate all word vectors (numpy arrays)\n",
    "words_array = np.stack(words_array, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs_list[0]), tfidf_weights[tfidf_mask].shape, words_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document vector (tfidf weighted average of word vector)\n",
    "np.dot(words_array, tfidf_weights[tfidf_mask]) / tfidf_weights[tfidf_mask].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed class and pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = word2vec.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        # if a word was never seen - it must be at least as infrequent\n",
    "        # as any of the known words - so the default idf is the max of \n",
    "        # known idf's\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "            lambda: max_idf,\n",
    "            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "                np.mean([self.word2vec[w] * self.word2weight[w]\n",
    "                         for w in words if w in self.word2vec] or\n",
    "                        [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfEmbeddingVectorizer(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.fit(docs_list, y_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec.transform(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = Pipeline([\n",
    "                (\"word2vec_vectorizer\", TfidfEmbeddingVectorizer(word2vec)),\n",
    "                (\"rf\", RandomForestRegressor())])\n",
    "\n",
    "parameters = {\n",
    "              'rf__n_estimators': [ 10 ],\n",
    "              'rf__min_samples_split': [ 3 ],\n",
    "              'rf__max_depth': [ 10 ]\n",
    "             }\n",
    "\n",
    "\n",
    "# set up grid search using all of my cores\n",
    "# I believe R^2 is the default scorer for Lasso\n",
    "gs_reg = GridSearchCV(reg, param_grid = parameters,\\\n",
    "                      scoring='neg_mean_squared_error',\\\n",
    "                      cv = 3 , n_jobs= -1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
