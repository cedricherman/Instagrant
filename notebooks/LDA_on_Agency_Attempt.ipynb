{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA, derive document-term matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Use bigrams and tf-idf. Remove investigator's names and stopwords in preprocessor, otherwise bigram such as 'the protein' will persist.\n",
    "Set min_df to reasonably low value, maybe 0.05 for 5% in order to reduce number of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom vectorizer is needed to lemmatize (see utilsvectorizer module)\n",
    "vecto = uv.CustomTfidfVectorizer(\n",
    "                            stop_words = 'english',\n",
    "                            min_df = 3,\n",
    "                            ngram_range=(1, 2),\n",
    "                            lowercase = True,\n",
    "                            strip_accents = 'unicode',\n",
    "                            token_pattern = r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b',\n",
    "                            binary = True,\n",
    "                            max_features = 2000\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce document-term matrix\n",
    "train_doc_term_matrix = vecto.fit_transform(train_docs.Abstract)\n",
    "train_doc_term_matrix.shape, type(train_doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply vectorization on test set\n",
    "test_doc_term_matrix = vecto.transform(test_docs.Abstract)\n",
    "test_doc_term_matrix.shape, type(test_doc_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Frequent Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recover words\n",
    "lemmatized_words = vecto.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum doc_term_matrix along rows for each column (i.e. word)\n",
    "word_freq = train_doc_term_matrix.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert word_freq from matrix to ndarray and flatten it (ravel is faster than flatten as it returns a view instead of a copy)\n",
    "# then create a pandas serie for display\n",
    "s_word_freq = pd.Series( np.asarray(word_freq).ravel(), index = lemmatized_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display top words\n",
    "s_word_freq.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display top words\n",
    "s_word_freq.sort_values(ascending=False).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Latent Dirichlet Analysis (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agency = df.Agency.value_counts().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run LDA with 7 components until perplexity reach plateau\n",
    "lda_all = LatentDirichletAllocation(n_components = num_agency,\n",
    "                                    max_iter = 500,\n",
    "                                    learning_method = 'batch',\n",
    "                                    evaluate_every = 10,\n",
    "                                    random_state = 7,\n",
    "                                    verbose = 1,\n",
    "                                    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lda_all.fit(train_doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle LDA model\n",
    "joblib.dump(lda_all, os.path.join(os.pardir,'models', 'lda_tfidf_full.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (gensim), derive document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard 0 amount, N/A abstract. Agency shall not be missing and only consider Phase I\n",
    "crit = (df.Agency == 'National Science Foundation') \\\n",
    "        & (df['Awards Year'].between(2010, 2013))\n",
    "df_analysis = df.loc[crit,['Abstract','Agency', 'title']]\n",
    "print(df_analysis.shape)\n",
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate abstract and title\n",
    "df_analysis['text'] = df_analysis.Abstract + df_analysis.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a custom `tokenizer` for Lemmatization with `spacy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(doc):\n",
    "    return [w.lemma_ for w in nlp(doc) if (not w.is_punct | w.is_space) ]\n",
    "# return [w.lemma_ for w in nlp(doc) if (not w.is_punct | w.is_space) & (w.pos_ in ['NOUN', 'ADJ', 'ADV']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # nlp(doc)\n",
    "# ex = nlp(df_analysis.text.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ex.similarity(nlp(df_analysis.text.iloc[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9198,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecto = TfidfVectorizer(\n",
    "                        min_df = 0.01,\n",
    "                        max_df = 0.8,\n",
    "                        ngram_range=(1, 2),\n",
    "                        stop_words = 'english',\n",
    "                        tokenizer = tokenizer,\n",
    "                        lowercase = True,\n",
    "                        strip_accents = 'unicode',\n",
    "#                         token_pattern = r'(?u)\\b[a-zA-Z][a-zA-Z]+\\b',\n",
    "                        binary = False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_train = vecto.fit_transform(df_analysis.text)\n",
    "# produce document-term matrix\n",
    "# dtm_train = vecto.fit_transform(X_train)\n",
    "# dtm_test = vecto.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9198, 1738), scipy.sparse.csr.csr_matrix)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_train.shape, type(dtm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = vecto.vocabulary_.get('technology', 'Nope')\n",
    "vecto.idf_[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = Sparse2Corpus(dtm_train, documents_columns=False)\n",
    "# test_corpus = Sparse2Corpus(dtm_test, documents_columns=False)\n",
    "\n",
    "id2word = pd.Series(vecto.get_feature_names()).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(id2word), id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20x more documents than topics\n",
    "num_topics = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T05:07:27.311042Z",
     "start_time": "2018-05-01T05:05:23.051642Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lda_gensim = LdaModel(corpus=train_corpus,\n",
    "                          num_topics = num_topics,\n",
    "                          id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T05:04:28.529642Z",
     "start_time": "2018-05-01T05:02:33.896Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "topics = lda_gensim.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coherence: List of tuples, one element per topic\n",
    "#            each element is also a tuple, 1st: list of (coherence score, term); 2nd: overall topic coherence\n",
    "coherence = lda_gensim.top_topics(corpus=train_corpus, coherence='u_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels = ['Topic {}'.format(i) for i in range(1, num_topics+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_coherence = []\n",
    "topic_words = pd.DataFrame()\n",
    "\n",
    "for t in range(len(coherence)):\n",
    "    # made up label\n",
    "    label = topic_labels[t]\n",
    "    # second element is the overall topic coherence\n",
    "    topic_coherence.append(coherence[t][1])\n",
    "    \n",
    "    # 1st element is a tuple \n",
    "    df_cohe = pd.DataFrame(coherence[t][0], columns=[(label, 'prob'), (label, 'term')])\n",
    "    df_cohe[(label, 'prob')] = df_cohe[(label, 'prob')].apply(lambda x: '{:.2%}'.format(x))\n",
    "    \n",
    "    topic_words = pd.concat([topic_words, df_cohe], axis=1)\n",
    "                      \n",
    "topic_words.columns = pd.MultiIndex.from_tuples(topic_words.columns)\n",
    "# pd.set_option('expand_frame_repr', False)\n",
    "print(topic_words.head())\n",
    "\n",
    "# plot overall topci coherence\n",
    "pd.Series(topic_coherence, index=topic_labels).plot.bar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization\n",
    "prepare_gensim(lda_gensim, train_corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim summarization\n",
    "TextRank works as follows:\n",
    "\n",
    "1. Pre-process the text: remove stop words and stem the remaining words.\n",
    "2. Create a graph where vertices (nodes) are sentences.\n",
    "3. Connect every sentence to every other sentence by an edge. The weight of the edge is how similar the two sentences are*.\n",
    "4. Run the PageRank algorithm on the graph.\n",
    "5. Pick the vertices(aka nodes that are sentences here) with the highest PageRank score\n",
    "\n",
    "*Gensimâ€™s TextRank uses [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25) function to see how similar the sentences are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_abstracts = df_analysis.text.str.cat(sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summarize(all_abstracts, word_count=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keywords(all_abstracts, words = 3, lemmatize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save LDA viz to standalone html\n",
    "pyLDAvis.save_html(\\\n",
    "    prepare(lda_all, train_doc_term_matrix, vecto), os.path.join(os.pardir, 'Small_Business_Award.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization\n",
    "prepare(lda_all, train_doc_term_matrix, vecto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lambda\n",
    "\n",
    "- **$\\lambda$ = 0**: how probable is a word to appear in a topic - words are ranked on lift P(word | topic) / P(word)\n",
    "- **$\\lambda$ = 1**: how exclusive is a word to a topic -  words are purely ranked on P(word | topic)\n",
    "\n",
    "The ranking formula is $\\lambda * P(\\text{word} \\vert \\text{topic}) + (1 - \\lambda) * \\text{lift}$\n",
    "\n",
    "User studies suggest $\\lambda = 0.6$ works for most people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
